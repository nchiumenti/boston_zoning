{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c976a71b-0cd9-4e9b-b9fc-bf084b174cd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T14:48:02.174633Z",
     "iopub.status.busy": "2022-10-21T14:48:02.174462Z",
     "iopub.status.idle": "2022-10-21T14:48:02.178792Z",
     "shell.execute_reply": "2022-10-21T14:48:02.178038Z",
     "shell.execute_reply.started": "2022-10-21T14:48:02.174578Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# File name:    \"zone_assignment.ipynb\"\n",
    "#\n",
    "# Project title:    Boston Affordable Housing project (visting scholar porject)\n",
    "#\n",
    "# Description:    This program takes the unique set of all warren group property\n",
    "#                 tax records in the MAPC region and assigns them to (1) a\n",
    "#                 schools attendance area <ncessch>, (2) a zone use type area\n",
    "#                 <zo_usety>, and (3) a left/right boundary id and regulation \n",
    "#                 type area <l_r_fid> and <reg_type>. The exported .csv file\n",
    "#                 is used in the closest_boundary_matches.py program.\n",
    "#\n",
    "# Inputs:    ./warren_MAPC_all_unique.dta\n",
    "#            ./sabs_unique_latlong.shp\n",
    "#            ./roads_mapc_union_sd_dissolved.shp\n",
    "#            ./zoning_atlas_latlong.shp\n",
    "#\n",
    "# Outputs:    ./zone_assignments_export.csv\n",
    "#             ./zone_assignments_log.txt\n",
    "#\n",
    "# Created:    10/18/2022\n",
    "# Updated:    10/21/2022\n",
    "#\n",
    "# Author:    Nicholas Chiumenti\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc37c806-689f-4f6c-9731-67b8cc778a72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T14:48:02.179774Z",
     "iopub.status.busy": "2022-10-21T14:48:02.179421Z",
     "iopub.status.idle": "2022-10-21T14:48:02.906552Z",
     "shell.execute_reply": "2022-10-21T14:48:02.905575Z",
     "shell.execute_reply.started": "2022-10-21T14:48:02.179755Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8968b18-768c-443d-915a-73a40781f36d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T14:48:02.909293Z",
     "iopub.status.busy": "2022-10-21T14:48:02.909006Z",
     "iopub.status.idle": "2022-10-21T14:48:02.913538Z",
     "shell.execute_reply": "2022-10-21T14:48:02.913051Z",
     "shell.execute_reply.started": "2022-10-21T14:48:02.909268Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running zone_assignments program...\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "print(\"Running zone_assignments program...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c227ad1-b351-4f22-9c61-ddffe3571697",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87280fa1-a3a9-43e2-8a15-601b2060907a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T14:48:02.914409Z",
     "iopub.status.busy": "2022-10-21T14:48:02.914242Z",
     "iopub.status.idle": "2022-10-21T14:48:02.928368Z",
     "shell.execute_reply": "2022-10-21T14:48:02.927943Z",
     "shell.execute_reply.started": "2022-10-21T14:48:02.914392Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full path to warren group property data\n",
    "data_path = \"/home/a1nfc04/Documents/boston_zoning_sdrive/data/warren/warren_MAPC_all_unique.dta\"\n",
    "\n",
    "# full path to school attendance area data\n",
    "schools_path = \"/home/a1nfc04/Documents/boston_zoning_sdrive/data/shapefiles/standardized/sabs_unique_latlong.shp\"\n",
    "\n",
    "# full path to zone area data (came from Amrita)\n",
    "zones_path = \"/home/a1nfc04/Documents/boston_zoning_sdrive/data/shapefiles/originals/roads_mapc_union_sd_dissolved.shp\"\n",
    "\n",
    "# full path to zone use type data (came from MAPC)\n",
    "zuse_path = \"/home/a1nfc04/Documents/boston_zoning_sdrive/data/shapefiles/standardized/zoning_atlas_latlong.shp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5f77e-b42b-4460-b297-c081c000c53e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load in initial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6394ceab-5cea-47cf-963c-777a14708bd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T14:48:02.929185Z",
     "iopub.status.busy": "2022-10-21T14:48:02.928994Z",
     "iopub.status.idle": "2022-10-21T14:48:45.292975Z",
     "shell.execute_reply": "2022-10-21T14:48:45.292485Z",
     "shell.execute_reply.started": "2022-10-21T14:48:02.929169Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading input datasets!\n"
     ]
    }
   ],
   "source": [
    "# import warren group property data\n",
    "data_df = pd.read_stata(data_path)\n",
    "\n",
    "# import school attendance areas\n",
    "schools_gdf = gpd.read_file(schools_path) \n",
    "\n",
    "# import zone areas\n",
    "zones_gdf = gpd.read_file(zones_path) # <-- this is the only one we use the original version for \n",
    "\n",
    "# import zone use type areas\n",
    "zuse_gdf = gpd.read_file(zuse_path)\n",
    "\n",
    "# convert property dataframe to geo-dataframe\n",
    "data_df = data_df[[\"prop_id\", \"cousub_name\", \"warren_latitude\", \"warren_longitude\"]]\n",
    "\n",
    "data_gdf = gpd.GeoDataFrame(data_df, geometry = gpd.points_from_xy(data_df[\"warren_longitude\"], data_df[\"warren_latitude\"]),\n",
    "                            crs = \"EPSG:4269\")\n",
    "\n",
    "# convert zone gdf to crs 4269\n",
    "zones_gdf.to_crs(\"EPSG:4269\", inplace = True)\n",
    "\n",
    "# error checks\n",
    "assert len(data_gdf) == 821237, \"incorrect observation count for data_gdf\"\n",
    "assert len(schools_gdf) == 231, \"incorrect observation count for schools_gdf\"\n",
    "assert len(zones_gdf) == 8719, \"incorrect observation count for zones_gdf\"\n",
    "assert len(zuse_gdf) == 1775, \"incorrect observation count for zuse_gdf\"\n",
    "\n",
    "assert data_gdf.crs == schools_gdf.crs, \"data_gdf crs does not match with schools_gdf\"\n",
    "assert data_gdf.crs == zones_gdf.crs, \"data_gdf crs does not match with zones_gdf\"\n",
    "assert data_gdf.crs == zuse_gdf.crs,\"data_gdf crs does not match with zuse_gdf\"\n",
    "\n",
    "print(\"Done loading input datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20758a-6c6c-45e3-b8dd-4da510ffb1f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assign school attendance areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92b16a5-5801-4f77-b5f3-9d326a3b12df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T14:48:45.293961Z",
     "iopub.status.busy": "2022-10-21T14:48:45.293763Z",
     "iopub.status.idle": "2022-10-21T14:49:07.067011Z",
     "shell.execute_reply": "2022-10-21T14:49:07.066468Z",
     "shell.execute_reply.started": "2022-10-21T14:48:45.293944Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done matching school attendance areas!\n"
     ]
    }
   ],
   "source": [
    "# spatially merge properties to school attendance areas\n",
    "merge_gdf1 = gpd.sjoin(data_gdf, schools_gdf, how=\"left\", op=\"within\")\n",
    "\n",
    "# drop all observations that match to >1 school attendance area\n",
    "merge_gdf1.drop_duplicates(subset=\"prop_id\", keep=False, inplace=True)\n",
    "assert len(merge_gdf1) == 811642, \"incorrect number of observatins in merge1_gdf, post dup drop\"\n",
    "\n",
    "# set ncessch as town name for open enrollment municipalities\n",
    "open_list = [\"ACTON\",\n",
    "             \"BOLTON\",\n",
    "             \"BOSTON\",\n",
    "             \"BOXBORO\",\n",
    "             \"ESSEX\",\n",
    "             \"MANCHESTER\",\n",
    "             \"SAUGUS\",\n",
    "             \"STOW\"]\n",
    "\n",
    "for x in open_list:\n",
    "    merge_gdf1.loc[(merge_gdf1[\"cousub_name\"] == x), \"ncessch\"] = x\n",
    "    \n",
    "# set ncessch as NaN for out-of-scope cities and towns\n",
    "nan_list = ['BELLINGHAM',\n",
    "            'BRAINTREE',\n",
    "            'BURLINGTON',\n",
    "            'CHELSEA',\n",
    "            'CONCORD',\n",
    "            'DANVERS',\n",
    "            'HAMILTON',\n",
    "            'HINGHAM',\n",
    "            'IPSWICH',\n",
    "            'LYNNFIELD',\n",
    "            'MEDFORD',\n",
    "            'MELROSE',\n",
    "            'NATICK',\n",
    "            'NORWOOD',\n",
    "            'PEABODY',\n",
    "            'QUINCY',\n",
    "            'READING',\n",
    "            'WATERTOWN',\n",
    "            'WENHAM',\n",
    "            'WILMINGTON',\n",
    "            'WINCHESTER',\n",
    "            'WOBURN']\n",
    "\n",
    "for x in nan_list:\n",
    "    merge_gdf1.loc[(merge_gdf1[\"cousub_name\"] == x), \"ncessch\"] = np.nan\n",
    "\n",
    "assert len(merge_gdf1[merge_gdf1[\"ncessch\"].isna()]) == 179256, \"incorrect number of missing ncessch observations\"\n",
    "\n",
    "# drop missing <ncessch> observations\n",
    "merge_gdf1 = merge_gdf1[merge_gdf1['ncessch'].notna()]\n",
    "\n",
    "# convert <ncessch> to string\n",
    "merge_gdf1[\"ncessch\"] = merge_gdf1[\"ncessch\"].astype(str)\n",
    "\n",
    "# trim dataset variables, save as school_matches_df\n",
    "school_matches_df = merge_gdf1[[\"prop_id\", \"ncessch\"]].copy()\n",
    "\n",
    "# final error checks\n",
    "assert len(school_matches_df) == 632386, \"incorrect number of observations in merge_gdf1\"\n",
    "assert school_matches_df[\"prop_id\"].nunique() == 632386, \"incorrect number of unique prop_ids\"\n",
    "assert school_matches_df[\"prop_id\"].nunique() == len(school_matches_df), \"number of observations in merge_gdf1 does not equal number of unique prop_ds\"\n",
    "assert school_matches_df[\"ncessch\"].nunique() == 233, \"incorrect number of unique ncessch ids\"\n",
    "assert all(school_matches_df[\"ncessch\"].apply(type) == str), \"not all values of ncessch are strings\"\n",
    "\n",
    "print(\"Done matching school attendance areas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ed4f1-dbd7-4c5b-9520-a0608943db5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assign zone use types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fbaa6b7-5d25-4d25-9741-8161ce26e17c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T14:49:07.068233Z",
     "iopub.status.busy": "2022-10-21T14:49:07.068003Z",
     "iopub.status.idle": "2022-10-21T14:50:28.697962Z",
     "shell.execute_reply": "2022-10-21T14:50:28.697460Z",
     "shell.execute_reply.started": "2022-10-21T14:49:07.068211Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/278991.1.jupyterhub.q/ipykernel_2968786/341982636.py:42: UserWarning: Geometry is in a geographic CRS. Results from 'distance' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  closest_zuse.loc[:,\"dist\"] = gpd.GeoSeries(closest_zuse[\"geometry_left\"], crs = \"EPSG:4269\").distance(gpd.GeoSeries(closest_zuse[\"geometry_zuse\"], crs = \"EPSG:4269\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done assigning zone use types!\n"
     ]
    }
   ],
   "source": [
    "# standardize city and town names\n",
    "zuse_gdf['muni'] = zuse_gdf['muni'].str.upper()\n",
    "zuse_gdf['muni'].replace({'MARLBOROUGH':'MARLBORO'}, inplace = True)\n",
    "zuse_gdf['muni'].replace({'FOXBOROUGH':'FOXBORO'}, inplace = True)\n",
    "zuse_gdf['muni'].replace({'SOUTHBOROUGH':'SOUTHBORO'}, inplace = True)\n",
    "zuse_gdf['muni'].replace({'BOXBOROUGH':'BOXBORO'}, inplace = True)\n",
    "\n",
    "# check that all towns in data_gdf are in zuse_gdf\n",
    "assert all([m2 in [m1 for m1 in zuse_gdf[\"muni\"].unique()] \n",
    "                  for m2 in data_gdf[\"cousub_name\"].unique()]\n",
    "          ), \"cities/towns are in data_df that are not in zuse_gdf\"\n",
    "\n",
    "assert all([m2 for m2 in zuse_gdf[\"muni\"].unique() \n",
    "            if m2 not in [m1 for m1 in data_gdf[\"cousub_name\"].unique()]]\n",
    "          ), \"cities/towns are in zuse_df that are not in data_gdf\"\n",
    "\n",
    "# assign zone use type areas\n",
    "merge_gdf2 = gpd.sjoin(data_gdf, zuse_gdf, how = \"left\", op = \"within\")\n",
    "\n",
    "# drop all observations that match to >1 zuse area\n",
    "merge_gdf2.drop_duplicates(subset = \"prop_id\", keep = False, inplace = True)\n",
    "assert len(merge_gdf2) == 821062, \"incorrect nubmer of observations in merge_gdf2 after duplicates drop\"\n",
    "\n",
    "# check the number of observations with missing zone use types\n",
    "missing_no = merge_gdf2['zo_usety'].isna().sum()\n",
    "assert missing_no == 42800, \"incorrect number of observations with missing zone use types\"\n",
    "\n",
    "## fill in missing zone use types based on closest zone use area\n",
    "# create dataframe of obs with missing zone use types\n",
    "missing_zuse = merge_gdf2.loc[merge_gdf2['zo_usety'].isna()]\n",
    "\n",
    "# merge missings dataframe with zuse_gdf based on city/town name\n",
    "missing_zuse = missing_zuse[[\"prop_id\", \"cousub_name\", \"warren_latitude\", \"warren_longitude\", \"geometry\"]]\n",
    "\n",
    "closest_zuse = missing_zuse.merge(zuse_gdf, \n",
    "                               how = \"left\",\n",
    "                               left_on = \"cousub_name\",\n",
    "                               right_on = \"muni\",\n",
    "                               suffixes = (\"_left\", \"_zuse\"))\n",
    "\n",
    "# calculate the dist between point (property) and polygon (zuse type area)\n",
    "closest_zuse.loc[:,\"dist\"] = gpd.GeoSeries(closest_zuse[\"geometry_left\"], crs = \"EPSG:4269\").distance(gpd.GeoSeries(closest_zuse[\"geometry_zuse\"], crs = \"EPSG:4269\"))\n",
    "\n",
    "# sort by prop_id and distance, keep 1st closest match\n",
    "closest_zuse.sort_values(by = [\"prop_id\", \"dist\"], ascending = True, inplace = True)\n",
    "closest_zuse = closest_zuse.groupby(\"prop_id\").head(1).reset_index(drop = True)\n",
    "assert len(closest_zuse) == 42800, \"incorrect number of observations in closest_zuse\"\n",
    "\n",
    "# flag the observations using closest match\n",
    "closest_zuse.loc[:, \"nan_change\"] = 1\n",
    "\n",
    "# merge closest matches back to main matches gdf\n",
    "merge_gdf2 = merge_gdf2.merge(closest_zuse, how = \"left\", on = \"prop_id\", suffixes = (\"\", \"_r\"))\n",
    "\n",
    "# set nan_change equal to zero if not 1\n",
    "merge_gdf2['nan_change'] = merge_gdf2['nan_change'].fillna(0)\n",
    "assert dict(merge_gdf2['nan_change'].value_counts()) == {0.0: 778262, 1.0: 42800}, \"incorrect value counts for nan_change\"\n",
    "\n",
    "# fill in zone use type if missing (nan_chane==1)\n",
    "merge_gdf2.loc[merge_gdf2[\"nan_change\"] == 1, \"zo_usety\"] = merge_gdf2[\"zo_usety_r\"]\n",
    "assert merge_gdf2[\"zo_usety\"].isna().sum() == 0, \"there are still missing zone use types in merge_gdf2\"\n",
    "\n",
    "# convert <zo_usety> to integer and then string\n",
    "merge_gdf2[\"zo_usety\"] = merge_gdf2[\"zo_usety\"].astype(int)\n",
    "merge_gdf2[\"zo_usety\"] = merge_gdf2[\"zo_usety\"].astype(str)\n",
    "\n",
    "# trim dataset variables, save as zuse_matches_df\n",
    "zuse_matches_df = merge_gdf2[[\"prop_id\", \"zo_usety\"]].copy()\n",
    "\n",
    "# final error checks\n",
    "assert len(zuse_matches_df) == 821062, \"incorrect number of observations in zuse_matches_df\"\n",
    "assert zuse_matches_df[\"prop_id\"].nunique() == 821062, \"incorrect number of unique prop_ids in zuse_matches_df\"\n",
    "assert zuse_matches_df[\"prop_id\"].nunique() == len(zuse_matches_df), \"number of observations and number of unique prop_ids does not match in zuse_matches_df\"\n",
    "assert zuse_matches_df[\"zo_usety\"].nunique() == 5, \"incorrect number of unique zo_usety ids in zuse_matches_df\"\n",
    "assert dict(zuse_matches_df[\"zo_usety\"].value_counts()) == {\"1\": 731070, \"3\": 62284, \"2\": 18075, \"0\": 5159, \"4\": 4474}, \"incorrect value counts for zo_usety\"\n",
    "assert all(zuse_matches_df[\"zo_usety\"].apply(type) == str), \"not all values of zo_usety are strings\"\n",
    "                       \n",
    "print(\"Done assigning zone use types!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba0c9e-cbe6-44db-b121-9b7289f02303",
   "metadata": {},
   "source": [
    "# Assign zoning regulatin area (l_r_fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ce8782-9e55-402b-a9de-49f538ee2588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T14:50:28.699198Z",
     "iopub.status.busy": "2022-10-21T14:50:28.698990Z",
     "iopub.status.idle": "2022-10-21T14:50:35.825345Z",
     "shell.execute_reply": "2022-10-21T14:50:35.824862Z",
     "shell.execute_reply.started": "2022-10-21T14:50:28.699178Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done assigning zone regulation types and l_r_fid!\n"
     ]
    }
   ],
   "source": [
    "# define l_r_fid as the index\n",
    "zones_gdf.loc[:,'l_r_fid'] = zones_gdf.index\n",
    "\n",
    "# spatial join with address points\n",
    "merge_gdf3 = gpd.sjoin(data_gdf, zones_gdf, how = \"left\", op = \"within\")\n",
    "\n",
    "# drop duplicated observations including originals\n",
    "merge_gdf3.drop_duplicates(subset = \"prop_id\", keep = False, inplace = True)\n",
    "assert len(merge_gdf3) == 821187, \"incorrect observation count for merge_gdf3 after duplicates drop\"\n",
    "\n",
    "# missing value error checks\n",
    "assert merge_gdf3[\"l_r_fid\"].isna().sum() == 187515, \"incorrect number of observations with missing l_r_fid in merge_gdf3\"\n",
    "assert merge_gdf3[\"reg_type\"].isna().sum() == 187515, \"incorrect number of observations with missing reg_type in merge_gdf3\"\n",
    "assert merge_gdf3[\"l_r_fid\"].isna().sum() == merge_gdf3[\"reg_type\"].isna().sum(), \"number of missing l_r_fid and reg_type do not match\"\n",
    "\n",
    "# drop observations with missing l_r_fid values\n",
    "merge_gdf3 = merge_gdf3[merge_gdf3[\"l_r_fid\"].notna()]\n",
    "\n",
    "# convert l_r_fid and reg_type to interger and then string\n",
    "merge_gdf3[[\"l_r_fid\", \"reg_type\"]] = merge_gdf3[[\"l_r_fid\", \"reg_type\"]].astype(int)\n",
    "merge_gdf3[[\"l_r_fid\", \"reg_type\"]] = merge_gdf3[[\"l_r_fid\", \"reg_type\"]].astype(str)\n",
    "\n",
    "# trim dataset variables, save as zone_matches_df\n",
    "zone_matches_df = merge_gdf3[[\"prop_id\", \"reg_type\", \"l_r_fid\"]].copy()\n",
    "\n",
    "# final error checks\n",
    "assert len(zone_matches_df) == 633672, \"incorrect number of observations in zone_matches_df\"\n",
    "assert zone_matches_df[\"prop_id\"].nunique() == 633672, \"incorrect number of unique prop_ids in zone_matches_df\"\n",
    "assert zone_matches_df[\"prop_id\"].nunique() == len(zone_matches_df), \"number of observations and number of unique prop_ids does not match in zone_matches_df\"\n",
    "assert zone_matches_df[\"l_r_fid\"].nunique() == 5963, \"incorrect number of unique l_r_fid ids in zone_matches_df\"\n",
    "assert zone_matches_df[\"reg_type\"].nunique() == 451, \"incorrect number of unique reg_type ids in zone_matches_df\"\n",
    "assert all(zone_matches_df[\"l_r_fid\"].apply(type) == str), \"not all values of l_r_fids are strings\"\n",
    "assert all(zone_matches_df[\"reg_type\"].apply(type) == str), \"not all values of reg_types are strings\"\n",
    "\n",
    "print(\"Done assigning zone regulation types and l_r_fid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6db6ba2-a624-4399-b264-0adee970de4e",
   "metadata": {},
   "source": [
    "# Combine assigments with base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7339bb9-5fd2-42fe-b084-5d77b11f7277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T14:50:35.826406Z",
     "iopub.status.busy": "2022-10-21T14:50:35.826213Z",
     "iopub.status.idle": "2022-10-21T14:50:38.841383Z",
     "shell.execute_reply": "2022-10-21T14:50:38.840891Z",
     "shell.execute_reply.started": "2022-10-21T14:50:35.826388Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done compiling final dataset for export!\n"
     ]
    }
   ],
   "source": [
    "# copy data_df to final_df\n",
    "final_df = data_df.copy()\n",
    "\n",
    "# merge with school attendance area matches\n",
    "final_df = final_df.merge(school_matches_df, how = \"outer\", on = \"prop_id\", indicator = True, validate=\"1:1\")\n",
    "final_df.rename(columns = {\"_merge\" : \"school_merge\"}, inplace = True)\n",
    "assert dict(final_df[\"school_merge\"].value_counts()) == {'both': 632386, 'left_only': 188851, 'right_only': 0}, \"incorrect merge of school_matches_df\"\n",
    "                                                         \n",
    "# merge with zone use type area matches\n",
    "final_df = final_df.merge(zuse_matches_df, how = \"outer\", on = \"prop_id\", indicator = True, validate = \"1:1\")\n",
    "final_df.rename(columns = {\"_merge\" : \"zuse_merge\"}, inplace = True)\n",
    "assert dict(final_df[\"zuse_merge\"].value_counts()) == {'both': 821062, 'left_only': 175, 'right_only': 0}, \"incorrect merge of school_matches_df\"\n",
    "                                                         \n",
    "# merge with zoning area id matches\n",
    "final_df = final_df.merge(zone_matches_df, how = \"outer\", on = \"prop_id\", indicator = True, validate = \"1:1\")\n",
    "final_df.rename(columns = {\"_merge\" : \"zone_merge\"}, inplace = True)\n",
    "assert dict(final_df[\"zone_merge\"].value_counts()) == {'both': 633672, 'left_only': 187565, 'right_only': 0}, \"incorrect merge of school_matches_df\"\n",
    "\n",
    "# drop geometry variable\n",
    "final_df.drop(columns = \"geometry\", inplace = True)\n",
    "\n",
    "# confirm that prop_id is all integers\n",
    "assert all(final_df[\"prop_id\"].apply(type) == int), \"not all values of prop_id are integers\"\n",
    "\n",
    "# confirm length of dataframe excluding nan values\n",
    "mask = (final_df[\"ncessch\"].notna()\n",
    "        & final_df[\"zo_usety\"].notna() \n",
    "        & final_df[\"l_r_fid\"].notna()\n",
    "        & final_df[\"reg_type\"].notna())\n",
    "\n",
    "assert len(final_df[mask]) == 618643, \"number of observations excluding missing values is not correct in final_df\"\n",
    "\n",
    "# confirm the distribution of values are the same\n",
    "error_check_dct = {'prop_id': \n",
    "                      {'count': 821237.0,\n",
    "                       'mean': 1452949.4701212926,\n",
    "                       'std': 1435185.2396116636,\n",
    "                       'min': 264.0,\n",
    "                       '25%': 414528.0,\n",
    "                       '50%': 884816.0,\n",
    "                       '75%': 1990482.0,\n",
    "                       'max': 5068039.0},\n",
    "                    'ncessch': \n",
    "                       {'count': 632386, \n",
    "                        'unique': 233, \n",
    "                        'top': 'BOSTON', \n",
    "                        'freq': 103463},\n",
    "                    'l_r_fid': \n",
    "                       {'count': 633672.0,\n",
    "                        'mean': 5198.722362673434,\n",
    "                        'std': 1565.8300020628192,\n",
    "                        'min': 0.0,\n",
    "                        '25%': 3844.0,\n",
    "                        '50%': 5299.0,\n",
    "                        '75%': 6293.0,\n",
    "                        'max': 8718.0},\n",
    "                    'zo_usety': \n",
    "                       {'count': 821062.0,\n",
    "                        'mean': 1.183793672097844,\n",
    "                        'std': 0.5890590627534994,\n",
    "                        'min': 0.0,\n",
    "                        '25%': 1.0,\n",
    "                        '50%': 1.0,\n",
    "                        '75%': 1.0,\n",
    "                        'max': 4.0},\n",
    "                   'reg_type': \n",
    "                       {'count': 633672.0,\n",
    "                        'mean': 163.150656806676,\n",
    "                        'std': 119.46259382918376,\n",
    "                        'min': 0.0,\n",
    "                        '25%': 45.0,\n",
    "                        '50%': 185.0,\n",
    "                        '75%': 245.0,\n",
    "                        'max': 542.0}}\n",
    "\n",
    "sum_stats_dct = {col : dict(final_df[final_df[col].notna()][col].astype(int).describe()) \n",
    "                 for col in [\"prop_id\", \"l_r_fid\", \"zo_usety\", \"reg_type\"]}\n",
    "\n",
    "sum_stats_dct[\"ncessch\"] = dict(final_df[final_df[\"ncessch\"].notna()][\"ncessch\"].describe())\n",
    "\n",
    "assert sum_stats_dct == error_check_dct, \"summary stats for variable list do not match expected values\"\n",
    "\n",
    "print(\"Done compiling final dataset for export!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e9b0bb-7465-4837-8477-e45a53467788",
   "metadata": {},
   "source": [
    "# Export data and save log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "165ad1e4-1c2b-473e-a8ce-ac628de28b0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-21T14:50:38.842710Z",
     "iopub.status.busy": "2022-10-21T14:50:38.842342Z",
     "iopub.status.idle": "2022-10-21T14:50:41.995504Z",
     "shell.execute_reply": "2022-10-21T14:50:41.995068Z",
     "shell.execute_reply.started": "2022-10-21T14:50:38.842688Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, 821,237 observations written!\n"
     ]
    }
   ],
   "source": [
    "# set log path\n",
    "log_path = \"/home/a1nfc04/Documents/boston_zoning_sdrive/python_programs/zone_assignments/zone_assignments_log.txt\"\n",
    "\n",
    "# save data paths\n",
    "save_file = \"zone_assignments_export.csv\"\n",
    "save_folder = \"/home/a1nfc04/Documents/boston_zoning_sdrive/python_programs/zone_assignments\"\n",
    "save_path = os.path.join(save_folder, save_file)\n",
    "\n",
    "# subdir for old exports\n",
    "old_saves_folder = \"/home/a1nfc04/Documents/boston_zoning_sdrive/python_programs/zone_assignments/old_export_versions\"\n",
    "\n",
    "# check if current save version exists, if so then move it to the old versions folder\n",
    "contents = [item for item in os.listdir(save_folder)]\n",
    "if save_file in contents:\n",
    "\n",
    "    # create previous saves folder is doesn't exist\n",
    "    if os.path.isdir(old_saves_folder) == False:\n",
    "        os.makedirs(old_saves_folder)\n",
    "    \n",
    "    # move file to sub-directory\n",
    "    old_file_path = os.path.join(old_saves_folder, save_file)\n",
    "    shutil.move(save_path, old_file_path)\n",
    "    \n",
    "    # rename the old file with creation date\n",
    "    create_date = datetime.fromtimestamp(os.path.getmtime(old_file_path)).strftime(\"_%Y-%m-%d\")\n",
    "    new_file_name = os.path.splitext(old_file_path)[0] + create_date +\".csv\"   \n",
    "    os.rename(old_file_path, new_file_name)\n",
    "    \n",
    "# export final_df dataset as .csv\n",
    "final_df.to_csv(save_path, index = False)\n",
    "\n",
    "# calcualte total program run time\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "duration_in_s = (duration.days * 24 * 60 * 60) + duration.seconds\n",
    "mins, secs = divmod(duration_in_s, 60)\n",
    "hours, mins = divmod(mins, 60)\n",
    "days, hours  = divmod(hours, 24)\n",
    "\n",
    "# write to log\n",
    "with open(log_path,'a') as file:\n",
    "    file.write(f\"Last run on {datetime.now().strftime('%D at %I:%M:%S %p')}\\n\")\n",
    "    file.write(f\"{len(final_df):,} observations written to: {save_path} \\n\")\n",
    "    file.write(f\"Total run time: {days} days, {hours:02} hours, {mins:02} minutes, {secs:02} seconds \\n\\n\")\n",
    "\n",
    "# Done!\n",
    "print(f\"Done, {len(final_df):,} observations written!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8d576-a936-42d8-81b4-6a3bb746c8da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boston_zoning",
   "language": "python",
   "name": "boston_zoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
