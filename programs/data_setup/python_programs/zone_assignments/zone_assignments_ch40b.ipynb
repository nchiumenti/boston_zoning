{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c976a71b-0cd9-4e9b-b9fc-bf084b174cd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T14:59:37.341873Z",
     "iopub.status.busy": "2022-11-21T14:59:37.341582Z",
     "iopub.status.idle": "2022-11-21T14:59:37.345195Z",
     "shell.execute_reply": "2022-11-21T14:59:37.344762Z",
     "shell.execute_reply.started": "2022-11-21T14:59:37.341821Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# File name:    \"zone_assignment_ch40b.ipynb\"\n",
    "#\n",
    "# Project title:    Boston Affordable Housing project (visting scholar porject)\n",
    "#\n",
    "# Description:    This program takes the unique set of CH40B property data\n",
    "#                 in the MAPC region and assigns them to (1) a\n",
    "#                 schools attendance area <ncessch>, (2) a zone use type area\n",
    "#                 <zo_usety>, and (3) a left/right boundary id and regulation \n",
    "#                 type area <l_r_fid> and <reg_type>. The exported .csv file\n",
    "#                 is used in the closest_boundary_matches_ch40b.py program.\n",
    "#\n",
    "# Inputs:    ./ch40b_mapc.dta\n",
    "#            ./sabs_unique_latlong.shp\n",
    "#            ./roads_mapc_union_sd_dissolved.shp\n",
    "#            ./zoning_atlas_latlong.shp \n",
    "#\n",
    "# Outputs:    ./zone_assignments_ch40b_export.csv\n",
    "#             ./zone_assignments_ch40b_log.txt\n",
    "#\n",
    "# Created:    11/18/2022\n",
    "# Updated:    11/18/2022\n",
    "#\n",
    "# Author:    Nicholas Chiumenti\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc37c806-689f-4f6c-9731-67b8cc778a72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T14:59:37.346069Z",
     "iopub.status.busy": "2022-11-21T14:59:37.345786Z",
     "iopub.status.idle": "2022-11-21T14:59:38.148115Z",
     "shell.execute_reply": "2022-11-21T14:59:38.146864Z",
     "shell.execute_reply.started": "2022-11-21T14:59:37.346052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8968b18-768c-443d-915a-73a40781f36d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T14:59:38.151430Z",
     "iopub.status.busy": "2022-11-21T14:59:38.151222Z",
     "iopub.status.idle": "2022-11-21T14:59:38.156053Z",
     "shell.execute_reply": "2022-11-21T14:59:38.155443Z",
     "shell.execute_reply.started": "2022-11-21T14:59:38.151376Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running zone_assignments program...\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "print(\"Running zone_assignments program...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c227ad1-b351-4f22-9c61-ddffe3571697",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87280fa1-a3a9-43e2-8a15-601b2060907a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T14:59:38.157212Z",
     "iopub.status.busy": "2022-11-21T14:59:38.156856Z",
     "iopub.status.idle": "2022-11-21T14:59:38.175071Z",
     "shell.execute_reply": "2022-11-21T14:59:38.174536Z",
     "shell.execute_reply.started": "2022-11-21T14:59:38.157192Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full path to warren group property data\n",
    "data_path = \"/home/a1nfc04/Documents/boston_zoning_sdrive/data/chapter40B/chapter40b_mapc.dta\"\n",
    "\n",
    "# full path to school attendance area data\n",
    "schools_path = \"/home/a1nfc04/Documents/boston_zoning_sdrive/data/shapefiles/standardized/sabs_unique_latlong.shp\"\n",
    "\n",
    "# full path to zone area data\n",
    "zones_path = \"/home/a1nfc04/Documents/boston_zoning_sdrive/data/shapefiles/originals/roads_mapc_union_sd_dissolved.shp\"\n",
    "\n",
    "# full path to zone use type data\n",
    "zuse_path = \"/home/a1nfc04/Documents/boston_zoning_sdrive/data/shapefiles/standardized/zoning_atlas_latlong.shp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5f77e-b42b-4460-b297-c081c000c53e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load in initial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6394ceab-5cea-47cf-963c-777a14708bd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T14:59:38.176096Z",
     "iopub.status.busy": "2022-11-21T14:59:38.175882Z",
     "iopub.status.idle": "2022-11-21T14:59:41.008823Z",
     "shell.execute_reply": "2022-11-21T14:59:41.008011Z",
     "shell.execute_reply.started": "2022-11-21T14:59:38.176079Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading input datasets!\n"
     ]
    }
   ],
   "source": [
    "# import warren group property data\n",
    "data_df = pd.read_stata(data_path)\n",
    "\n",
    "# import school attendance areas\n",
    "schools_gdf = gpd.read_file(schools_path) \n",
    "\n",
    "# import zone areas\n",
    "zones_gdf = gpd.read_file(zones_path) # <-- this is the only one we use the original version for \n",
    "\n",
    "# import zone use type areas\n",
    "zuse_gdf = gpd.read_file(zuse_path)\n",
    "\n",
    "# convert property dataframe to geo-dataframe\n",
    "data_df[\"cousub_name\"] = data_df[\"ch40b_city\"]\n",
    "data_df = data_df[[\"unique_id\", \"ch40b_id\", \"cousub_name\", \"ch40b_city\", \"ch40b_lat\", \"ch40b_lon\"]]\n",
    "\n",
    "data_gdf = gpd.GeoDataFrame(data_df, geometry = gpd.points_from_xy(data_df[\"ch40b_lon\"], data_df[\"ch40b_lat\"]),\n",
    "                            crs = \"EPSG:4269\")\n",
    "\n",
    "# convert zone gdf to crs 4269\n",
    "zones_gdf.to_crs(\"EPSG:4269\", inplace = True)\n",
    "\n",
    "# error checks\n",
    "assert len(data_gdf) == 1932, \"incorrect observation count for data_gdf\"\n",
    "assert len(schools_gdf) == 231, \"incorrect observation count for schools_gdf\"\n",
    "assert len(zones_gdf) == 8719, \"incorrect observation count for zones_gdf\"\n",
    "assert len(zuse_gdf) == 1775, \"incorrect observation count for zuse_gdf\"\n",
    "\n",
    "assert data_gdf.crs == schools_gdf.crs, \"data_gdf crs does not match with schools_gdf\"\n",
    "assert data_gdf.crs == zones_gdf.crs, \"data_gdf crs does not match with zones_gdf\"\n",
    "assert data_gdf.crs == zuse_gdf.crs,\"data_gdf crs does not match with zuse_gdf\"\n",
    "\n",
    "print(\"Done loading input datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20758a-6c6c-45e3-b8dd-4da510ffb1f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assign school attendance areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92b16a5-5801-4f77-b5f3-9d326a3b12df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T14:59:41.010515Z",
     "iopub.status.busy": "2022-11-21T14:59:41.010215Z",
     "iopub.status.idle": "2022-11-21T14:59:41.208840Z",
     "shell.execute_reply": "2022-11-21T14:59:41.208145Z",
     "shell.execute_reply.started": "2022-11-21T14:59:41.010489Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done matching school attendance areas!\n"
     ]
    }
   ],
   "source": [
    "# spatially merge properties to school attendance areas\n",
    "merge_gdf1 = gpd.sjoin(data_gdf, schools_gdf, how=\"left\", op=\"within\")\n",
    "\n",
    "# drop all observations that match to >1 school attendance area\n",
    "merge_gdf1.drop_duplicates(subset=\"unique_id\", keep=False, inplace=True)\n",
    "# assert len(merge_gdf1) == 811642, \"incorrect number of observatins in merge1_gdf, post dup drop\"\n",
    "\n",
    "# set ncessch as town name for open enrollment municipalities\n",
    "open_list = [\"ACTON\",\n",
    "             \"BOLTON\",\n",
    "             \"BOSTON\",\n",
    "             \"BOXBORO\",\n",
    "             \"ESSEX\",\n",
    "             \"MANCHESTER\",\n",
    "             \"SAUGUS\",\n",
    "             \"STOW\"]\n",
    "\n",
    "for x in open_list:\n",
    "    merge_gdf1.loc[(merge_gdf1[\"cousub_name\"] == x), \"ncessch\"] = x\n",
    "    \n",
    "# set ncessch as NaN for out-of-scope cities and towns\n",
    "nan_list = ['BELLINGHAM',\n",
    "            'BRAINTREE',\n",
    "            'BURLINGTON',\n",
    "            'CHELSEA',\n",
    "            'CONCORD',\n",
    "            'DANVERS',\n",
    "            'HAMILTON',\n",
    "            'HINGHAM',\n",
    "            'IPSWICH',\n",
    "            'LYNNFIELD',\n",
    "            'MEDFORD',\n",
    "            'MELROSE',\n",
    "            'NATICK',\n",
    "            'NORWOOD',\n",
    "            'PEABODY',\n",
    "            'QUINCY',\n",
    "            'READING',\n",
    "            'WATERTOWN',\n",
    "            'WENHAM',\n",
    "            'WILMINGTON',\n",
    "            'WINCHESTER',\n",
    "            'WOBURN']\n",
    "\n",
    "for x in nan_list:\n",
    "    merge_gdf1.loc[(merge_gdf1[\"cousub_name\"] == x), \"ncessch\"] = np.nan\n",
    "\n",
    "assert len(merge_gdf1[merge_gdf1[\"ncessch\"].isna()]) == 626, \"incorrect number of missing ncessch observations\"\n",
    "\n",
    "# drop missing <ncessch> observations\n",
    "merge_gdf1 = merge_gdf1[merge_gdf1['ncessch'].notna()]\n",
    "\n",
    "# convert <ncessch> to string\n",
    "merge_gdf1[\"ncessch\"] = merge_gdf1[\"ncessch\"].astype(str)\n",
    "\n",
    "# trim dataset variables, save as school_matches_df\n",
    "school_matches_df = merge_gdf1[[\"unique_id\", \"cousub_name\", \"ncessch\"]].copy()\n",
    "\n",
    "# final error checks\n",
    "assert len(school_matches_df) == 1261, \"incorrect number of observations in school_matches_df\"\n",
    "assert school_matches_df[\"unique_id\"].nunique() == 1261, \"incorrect number of unique prop_ids\"\n",
    "assert school_matches_df[\"unique_id\"].nunique() == len(school_matches_df), \"number of observations in merge_gdf1 does not equal number of unique prop_ds\"\n",
    "assert school_matches_df[\"ncessch\"].nunique() == 185, \"incorrect number of unique ncessch ids\"\n",
    "assert all(school_matches_df[\"ncessch\"].apply(type) == str), \"not all values of ncessch are strings\"\n",
    "\n",
    "print(\"Done matching school attendance areas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ed4f1-dbd7-4c5b-9520-a0608943db5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assign zone use types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fbaa6b7-5d25-4d25-9741-8161ce26e17c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T14:59:41.210193Z",
     "iopub.status.busy": "2022-11-21T14:59:41.209946Z",
     "iopub.status.idle": "2022-11-21T14:59:42.719432Z",
     "shell.execute_reply": "2022-11-21T14:59:42.718657Z",
     "shell.execute_reply.started": "2022-11-21T14:59:41.210173Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/279356.1.jupyterhub.q/ipykernel_1130853/213750876.py:42: UserWarning: Geometry is in a geographic CRS. Results from 'distance' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  closest_zuse.loc[:,\"dist\"] = gpd.GeoSeries(closest_zuse[\"geometry_left\"], crs = \"EPSG:4269\").distance(gpd.GeoSeries(closest_zuse[\"geometry_zuse\"], crs = \"EPSG:4269\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done assigning zone use types!\n"
     ]
    }
   ],
   "source": [
    "# standardize city and town names\n",
    "zuse_gdf['muni'] = zuse_gdf['muni'].str.upper()\n",
    "zuse_gdf['muni'].replace({'MARLBOROUGH':'MARLBORO'}, inplace = True)\n",
    "zuse_gdf['muni'].replace({'FOXBOROUGH':'FOXBORO'}, inplace = True)\n",
    "zuse_gdf['muni'].replace({'SOUTHBOROUGH':'SOUTHBORO'}, inplace = True)\n",
    "zuse_gdf['muni'].replace({'BOXBOROUGH':'BOXBORO'}, inplace = True)\n",
    "\n",
    "# check that all towns in data_gdf are in zuse_gdf\n",
    "assert all([m2 in [m1 for m1 in zuse_gdf[\"muni\"].unique()] \n",
    "                  for m2 in data_gdf[\"cousub_name\"].unique()]\n",
    "          ), \"cities/towns are in data_df that are not in zuse_gdf\"\n",
    "\n",
    "assert all([m2 for m2 in zuse_gdf[\"muni\"].unique() \n",
    "            if m2 not in [m1 for m1 in data_gdf[\"cousub_name\"].unique()]]\n",
    "          ), \"cities/towns are in zuse_df that are not in data_gdf\"\n",
    "\n",
    "# assign zone use type areas\n",
    "merge_gdf2 = gpd.sjoin(data_gdf, zuse_gdf, how = \"left\", op = \"within\")\n",
    "\n",
    "# drop all observations that match to >1 zuse area\n",
    "merge_gdf2.drop_duplicates(subset = \"unique_id\", keep = False, inplace = True)\n",
    "assert len(merge_gdf2) == 1931, \"incorrect nubmer of observations in merge_gdf2 after duplicates drop\"\n",
    "\n",
    "# check the number of observations with missing zone use types\n",
    "missing_no = merge_gdf2['zo_usety'].isna().sum()\n",
    "assert missing_no == 561, \"incorrect number of observations with missing zone use types\"\n",
    "\n",
    "## fill in missing zone use types based on closest zone use area\n",
    "# create dataframe of obs with missing zone use types\n",
    "missing_zuse = merge_gdf2.loc[merge_gdf2['zo_usety'].isna()]\n",
    "\n",
    "# merge missings dataframe with zuse_gdf based on city/town name\n",
    "missing_zuse = missing_zuse[[\"unique_id\", \"cousub_name\", \"ch40b_lat\", \"ch40b_lon\", \"geometry\"]]\n",
    "\n",
    "closest_zuse = missing_zuse.merge(zuse_gdf, \n",
    "                               how = \"left\",\n",
    "                               left_on = \"cousub_name\",\n",
    "                               right_on = \"muni\",\n",
    "                               suffixes = (\"_left\", \"_zuse\"))\n",
    "\n",
    "# calculate the dist between point (property) and polygon (zuse type area)\n",
    "closest_zuse.loc[:,\"dist\"] = gpd.GeoSeries(closest_zuse[\"geometry_left\"], crs = \"EPSG:4269\").distance(gpd.GeoSeries(closest_zuse[\"geometry_zuse\"], crs = \"EPSG:4269\"))\n",
    "\n",
    "# sort by prop_id and distance, keep 1st closest match\n",
    "closest_zuse.sort_values(by = [\"unique_id\", \"dist\"], ascending = True, inplace = True)\n",
    "closest_zuse = closest_zuse.groupby(\"unique_id\").head(1).reset_index(drop = True)\n",
    "assert len(closest_zuse) == 561, \"incorrect number of observations in closest_zuse\"\n",
    "\n",
    "# flag the observations using closest match\n",
    "closest_zuse.loc[:, \"nan_change\"] = 1\n",
    "\n",
    "# merge closest matches back to main matches gdf\n",
    "merge_gdf2 = merge_gdf2.merge(closest_zuse, how = \"left\", on = \"unique_id\", suffixes = (\"\", \"_r\"))\n",
    "\n",
    "# set nan_change equal to zero if not 1\n",
    "merge_gdf2['nan_change'] = merge_gdf2['nan_change'].fillna(0)\n",
    "assert dict(merge_gdf2['nan_change'].value_counts()) == {0.0: 1370, 1.0: 561}, \"incorrect value counts for nan_change\"\n",
    "\n",
    "# fill in zone use type if missing (nan_chane==1)\n",
    "merge_gdf2.loc[merge_gdf2[\"nan_change\"] == 1, \"zo_usety\"] = merge_gdf2[\"zo_usety_r\"]\n",
    "assert merge_gdf2[\"zo_usety\"].isna().sum() == 0, \"there are still missing zone use types in merge_gdf2\"\n",
    "\n",
    "# convert <zo_usety> to integer and then string\n",
    "merge_gdf2[\"zo_usety\"] = merge_gdf2[\"zo_usety\"].astype(int)\n",
    "merge_gdf2[\"zo_usety\"] = merge_gdf2[\"zo_usety\"].astype(str)\n",
    "\n",
    "# trim dataset variables, save as zuse_matches_df\n",
    "zuse_matches_df = merge_gdf2[[\"unique_id\", \"zo_usety\"]].copy()\n",
    "\n",
    "# final error checks\n",
    "assert len(zuse_matches_df) == 1931, \"incorrect number of observations in zuse_matches_df\"\n",
    "assert zuse_matches_df[\"unique_id\"].nunique() == 1931, \"incorrect number of unique prop_ids in zuse_matches_df\"\n",
    "assert zuse_matches_df[\"unique_id\"].nunique() == len(zuse_matches_df), \"number of observations and number of unique prop_ids does not match in zuse_matches_df\"\n",
    "assert zuse_matches_df[\"zo_usety\"].nunique() == 5, \"incorrect number of unique zo_usety ids in zuse_matches_df\"\n",
    "assert dict(zuse_matches_df[\"zo_usety\"].value_counts()) == {'1': 1083, '3': 531, '2': 244, '0': 37, '4': 36}, \"incorrect value counts for zo_usety\"\n",
    "assert all(zuse_matches_df[\"zo_usety\"].apply(type) == str), \"not all values of zo_usety are strings\"\n",
    "                       \n",
    "print(\"Done assigning zone use types!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba0c9e-cbe6-44db-b121-9b7289f02303",
   "metadata": {},
   "source": [
    "# Assign zoning regulatin area (l_r_fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ce8782-9e55-402b-a9de-49f538ee2588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T14:59:42.720661Z",
     "iopub.status.busy": "2022-11-21T14:59:42.720420Z",
     "iopub.status.idle": "2022-11-21T14:59:43.468404Z",
     "shell.execute_reply": "2022-11-21T14:59:43.467691Z",
     "shell.execute_reply.started": "2022-11-21T14:59:42.720636Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done assigning zone regulation types and l_r_fid!\n"
     ]
    }
   ],
   "source": [
    "# define l_r_fid as the index\n",
    "zones_gdf.loc[:,'l_r_fid'] = zones_gdf.index\n",
    "\n",
    "# spatial join with address points\n",
    "merge_gdf3 = gpd.sjoin(data_gdf, zones_gdf, how = \"left\", op = \"within\")\n",
    "\n",
    "# drop duplicated observations including originals\n",
    "merge_gdf3.drop_duplicates(subset = \"unique_id\", keep = False, inplace = True)\n",
    "assert len(merge_gdf3) == 1931, \"incorrect observation count for merge_gdf3 after duplicates drop\"\n",
    "\n",
    "# missing value error checks\n",
    "assert merge_gdf3[\"l_r_fid\"].isna().sum() == 597, \"incorrect number of observations with missing l_r_fid in merge_gdf3\"\n",
    "assert merge_gdf3[\"reg_type\"].isna().sum() == 597, \"incorrect number of observations with missing reg_type in merge_gdf3\"\n",
    "assert merge_gdf3[\"l_r_fid\"].isna().sum() == merge_gdf3[\"reg_type\"].isna().sum(), \"number of missing l_r_fid and reg_type do not match\"\n",
    "\n",
    "# drop observations with missing l_r_fid values\n",
    "merge_gdf3 = merge_gdf3[merge_gdf3[\"l_r_fid\"].notna()]\n",
    "\n",
    "# convert l_r_fid and reg_type to interger and then string\n",
    "merge_gdf3[[\"l_r_fid\", \"reg_type\"]] = merge_gdf3[[\"l_r_fid\", \"reg_type\"]].astype(int)\n",
    "merge_gdf3[[\"l_r_fid\", \"reg_type\"]] = merge_gdf3[[\"l_r_fid\", \"reg_type\"]].astype(str)\n",
    "\n",
    "# trim dataset variables, save as zone_matches_df\n",
    "zone_matches_df = merge_gdf3[[\"unique_id\", \"reg_type\", \"l_r_fid\"]].copy()\n",
    "\n",
    "# final error checks\n",
    "assert len(zone_matches_df) == 1334, \"incorrect number of observations in zone_matches_df\"\n",
    "assert zone_matches_df[\"unique_id\"].nunique() == 1334, \"incorrect number of unique prop_ids in zone_matches_df\"\n",
    "assert zone_matches_df[\"unique_id\"].nunique() == len(zone_matches_df), \"number of observations and number of unique prop_ids does not match in zone_matches_df\"\n",
    "assert zone_matches_df[\"l_r_fid\"].nunique() == 532, \"incorrect number of unique l_r_fid ids in zone_matches_df\"\n",
    "assert zone_matches_df[\"reg_type\"].nunique() == 187, \"incorrect number of unique reg_type ids in zone_matches_df\"\n",
    "assert all(zone_matches_df[\"l_r_fid\"].apply(type) == str), \"not all values of l_r_fids are strings\"\n",
    "assert all(zone_matches_df[\"reg_type\"].apply(type) == str), \"not all values of reg_types are strings\"\n",
    "\n",
    "print(\"Done assigning zone regulation types and l_r_fid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6db6ba2-a624-4399-b264-0adee970de4e",
   "metadata": {},
   "source": [
    "# Combine assigments with base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7339bb9-5fd2-42fe-b084-5d77b11f7277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T14:59:43.469996Z",
     "iopub.status.busy": "2022-11-21T14:59:43.469692Z",
     "iopub.status.idle": "2022-11-21T14:59:43.519087Z",
     "shell.execute_reply": "2022-11-21T14:59:43.518472Z",
     "shell.execute_reply.started": "2022-11-21T14:59:43.469973Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done compiling final dataset for export!\n"
     ]
    }
   ],
   "source": [
    "# copy data_df to final_df\n",
    "final_df = data_df.copy()\n",
    "\n",
    "# merge with school attendance area matches\n",
    "final_df = final_df.merge(school_matches_df, how = \"outer\", on = \"unique_id\", indicator = True, validate=\"1:1\")\n",
    "final_df.rename(columns = {\"_merge\" : \"school_merge\"}, inplace = True)\n",
    "assert dict(final_df[\"school_merge\"].value_counts()) == {'both': 1261, 'left_only': 671, 'right_only': 0}, \"incorrect merge of school_matches_df\"\n",
    "                                                         \n",
    "# merge with zone use type area matches\n",
    "final_df = final_df.merge(zuse_matches_df, how = \"outer\", on = \"unique_id\", indicator = True, validate = \"1:1\")\n",
    "final_df.rename(columns = {\"_merge\" : \"zuse_merge\"}, inplace = True)\n",
    "assert dict(final_df[\"zuse_merge\"].value_counts()) == {'both': 1931, 'left_only': 1, 'right_only': 0}, \"incorrect merge of zuse_matches_df\"\n",
    "                                                         \n",
    "# merge with zoning area id matches\n",
    "final_df = final_df.merge(zone_matches_df, how = \"outer\", on = \"unique_id\", indicator = True, validate = \"1:1\")\n",
    "final_df.rename(columns = {\"_merge\" : \"zone_merge\"}, inplace = True)\n",
    "assert dict(final_df[\"zone_merge\"].value_counts()) == {'both': 1334, 'left_only': 598, 'right_only': 0}, \"incorrect merge of zone_matches_df\"\n",
    "\n",
    "# drop geometry variable\n",
    "final_df.drop(columns = \"geometry\", inplace = True)\n",
    "\n",
    "# rename cousub_name\n",
    "final_df.rename(columns = {\"cousub_name_x\": \"cousub_name\"}, inplace = True)\n",
    "final_df.drop(columns = \"cousub_name_y\", inplace = True)\n",
    "\n",
    "# confirm that prop_id is all integers\n",
    "final_df[\"unique_id\"] = final_df[\"unique_id\"].astype(int).astype(str)\n",
    "assert all(final_df[\"unique_id\"].apply(type) == str), \"not all values of unique_id are integers\"\n",
    "\n",
    "# confirm length of dataframe excluding nan values\n",
    "mask = (final_df[\"ncessch\"].notna()\n",
    "        & final_df[\"zo_usety\"].notna() \n",
    "        & final_df[\"l_r_fid\"].notna()\n",
    "        & final_df[\"reg_type\"].notna())\n",
    "\n",
    "assert len(final_df[mask]) == 1238, \"number of observations excluding missing values is not correct in final_df\"\n",
    "\n",
    "# # confirm the distribution of values are the same\n",
    "error_check_dct = {'unique_id': {'count': 1932.0, 'mean': 1714.621118, 'std': 1017.035188, 'min': 1.0, '25%': 892.75, '50%': 1616.0, '75%': 2587.25, 'max': 3475.0},\n",
    "                   'ch40b_id': {'count': 1932.0, 'mean': 4639.414596, 'std': 3784.237095, 'min': 10.0, '25%': 1606.0, '50%': 2804.0, '75%': 9197.0, 'max': 10582.0},\n",
    "                   'l_r_fid': {'count': 1334.0, 'mean': 5996.107946, 'std': 1685.203846, 'min': 167.0, '25%': 4611.0, '50%': 6184.0, '75%': 7422.0, 'max': 8711.0},\n",
    "                   'zo_usety': {'count': 1931.0, 'mean': 1.713102, 'std': 0.951426, 'min': 0.0, '25%': 1.0, '50%': 1.0, '75%': 3.0, 'max': 4.0},\n",
    "                   'reg_type': {'count': 1334.0, 'mean': 226.801349, 'std': 132.274455, 'min': 0.0, '25%': 90.0, '50%': 241.0, '75%': 316.0, 'max': 537.0},\n",
    "                   'ncessch': {'count': 1261, 'unique': 185, 'top': '250327000020', 'freq': 149}\n",
    "                  }\n",
    "\n",
    "sum_stats_dct = {col : dict(round(final_df[final_df[col].notna()][col].astype(int).describe(),6)) \n",
    "                 for col in [\"unique_id\", \"ch40b_id\", \"l_r_fid\", \"zo_usety\", \"reg_type\"]}\n",
    "\n",
    "sum_stats_dct[\"ncessch\"] = dict(final_df[final_df[\"ncessch\"].notna()][\"ncessch\"].describe())\n",
    "\n",
    "assert sum_stats_dct == error_check_dct, \"summary stats for variable list do not match expected values\"\n",
    "\n",
    "print(\"Done compiling final dataset for export!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e9b0bb-7465-4837-8477-e45a53467788",
   "metadata": {},
   "source": [
    "# Export data and save log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "165ad1e4-1c2b-473e-a8ce-ac628de28b0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T14:59:43.552707Z",
     "iopub.status.busy": "2022-11-21T14:59:43.552480Z",
     "iopub.status.idle": "2022-11-21T14:59:43.589442Z",
     "shell.execute_reply": "2022-11-21T14:59:43.588885Z",
     "shell.execute_reply.started": "2022-11-21T14:59:43.552689Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, 1,932 observations written!\n"
     ]
    }
   ],
   "source": [
    "# set log path\n",
    "log_path = \"/home/a1nfc04/Documents/boston_zoning_sdrive/python_programs/zone_assignments/zone_assignments_ch40b_log.txt\"\n",
    "\n",
    "# save data paths\n",
    "save_file = \"zone_assignments_ch40b_export.csv\"\n",
    "save_folder = \"/home/a1nfc04/Documents/boston_zoning_sdrive/python_programs/zone_assignments\"\n",
    "save_path = os.path.join(save_folder, save_file)\n",
    "\n",
    "# subdir for old exports\n",
    "old_saves_folder = \"/home/a1nfc04/Documents/boston_zoning_sdrive/python_programs/zone_assignments/old_export_versions\"\n",
    "\n",
    "# check if current save version exists, if so then move it to the old versions folder\n",
    "contents = [item for item in os.listdir(save_folder)]\n",
    "if save_file in contents:\n",
    "\n",
    "    # create previous saves folder is doesn't exist\n",
    "    if os.path.isdir(old_saves_folder) == False:\n",
    "        os.makedirs(old_saves_folder)\n",
    "    \n",
    "    # move file to sub-directory\n",
    "    old_file_path = os.path.join(old_saves_folder, save_file)\n",
    "    shutil.move(save_path, old_file_path)\n",
    "    \n",
    "    # rename the old file with creation date\n",
    "    create_date = datetime.fromtimestamp(os.path.getmtime(old_file_path)).strftime(\"_%Y-%m-%d\")\n",
    "    new_file_name = os.path.splitext(old_file_path)[0] + create_date +\".csv\"   \n",
    "    os.rename(old_file_path, new_file_name)\n",
    "    \n",
    "# export final_df dataset as .csv\n",
    "final_df.to_csv(save_path, index = False)\n",
    "\n",
    "# calcualte total program run time\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "duration_in_s = (duration.days * 24 * 60 * 60) + duration.seconds\n",
    "mins, secs = divmod(duration_in_s, 60)\n",
    "hours, mins = divmod(mins, 60)\n",
    "days, hours  = divmod(hours, 24)\n",
    "\n",
    "# write to log\n",
    "with open(log_path,'a') as file:\n",
    "    file.write(f\"Last run on {datetime.now().strftime('%D at %I:%M:%S %p')}\\n\")\n",
    "    file.write(f\"{len(final_df):,} observations written to: {save_path} \\n\")\n",
    "    file.write(f\"Total run time: {days} days, {hours:02} hours, {mins:02} minutes, {secs:02} seconds \\n\\n\")\n",
    "\n",
    "# Done!\n",
    "print(f\"Done, {len(final_df):,} observations written!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boston_zoning",
   "language": "python",
   "name": "boston_zoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
